# -*- coding: utf-8 -*-
"""working lora project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QZXrNEIMQ3-f2SkgBk0fDdg0l85_FE4s
"""

import torch

def check_gpu():
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        return f"GPU available: {gpu_name}"
    else:
        return "No GPU available"

print(check_gpu())

!pip install -q transformers datasets peft accelerate

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Step 3: Load Dataset and Tokenizer
from datasets import load_dataset
from transformers import AutoTokenizer

dataset = load_dataset("imdb")
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

def tokenize_fn(examples):
    return tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=512
    )

tokenized_dataset = dataset.map(tokenize_fn, batched=True)
train_dataset = tokenized_dataset["train"].shuffle().select(range(2500))  # Small subset for faster training
test_dataset = tokenized_dataset["test"].shuffle().select(range(500))

# Step 4: Load Model and Apply LoRA Configuration
from transformers import AutoModelForSequenceClassification
from peft import LoraConfig, get_peft_model

model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=2
).to(device)

lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_lin", "v_lin"],
    lora_dropout=0.1,
    bias="none"
)

peft_model = get_peft_model(model, lora_config)
peft_model.print_trainable_parameters()

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    logging_dir="./results/logs",
    logging_steps=50,                          # Reduced logging frequency
    evaluation_strategy="epoch",              # Keep evaluation per epoch
    save_strategy="epoch",                    # Save model per epoch
    per_device_train_batch_size=8,            # Safer for T4 GPU
    per_device_eval_batch_size=8,
    num_train_epochs=4,                       # Slightly more training
    learning_rate=2e-5,                       # More stable learning rate
    report_to="none"                          # No external logging
)

# Step 6: Initialize Trainer and Train
from transformers import Trainer

trainer = Trainer(
    model=peft_model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)

trainer.train()

# Step 7: Test Predictions
def predict_sentiment(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True).to(device)
    with torch.no_grad():
        outputs = peft_model(**inputs)
    return "Positive" if torch.argmax(outputs.logits).item() == 1 else "Negative"

print("\nSample predictions:")
print("1.", predict_sentiment("This movie was absolutely fantastic!"))  # Positive
print("2.", predict_sentiment("Worst film I've ever seen."))            # Negative

# Step 7: Test Predictions
def predict_sentiment(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True).to(device)
    with torch.no_grad():
        outputs = peft_model(**inputs)
    return "Positive" if torch.argmax(outputs.logits).item() == 1 else "Negative"

print("\nSample predictions:")
print("1.", predict_sentiment("This movie was good."))  # Positive
print("2.", predict_sentiment("The movie is not really good"))            # Negative

# Step 7: Test Predictions
def predict_sentiment(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True).to(device)
    with torch.no_grad():
        outputs = peft_model(**inputs)
    return "Positive" if torch.argmax(outputs.logits).item() == 1 else "Negative"

print("\nSample predictions:")
print("1.", predict_sentiment("This movie was horrible"))  # Positive
print("2.", predict_sentiment("best movie ever"))            # Negative



